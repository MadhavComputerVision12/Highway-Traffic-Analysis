# -*- coding: utf-8 -*-
"""Copy of DeepLearningSpeed.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GngQWb51CeDJqilqhFEbQGFobi2fWeOS

# **Mounting Drive**
"""

from google.colab import drive
drive.mount('/content/drive')

"""# **Convert 'xlsx' to 'csv'**"""

import pandas as pd

# Read the XLSX file into a pandas DataFrame
df = pd.read_excel('/content/drive/MyDrive/data_2.xlsx')

# Save the DataFrame as a CSV file
df.to_csv('output.csv', index=False)

"""# ***Reading 'csv' file***"""

data=pd.read_csv('output.csv')

"""# **Information of Data**"""

data.columns

data.info

import numpy as np
import matplotlib.pyplot as plt

plt.plot(data['VDS_1_Speed (mph)'][:100])

"""# **Creating Dataset**"""

from sklearn.preprocessing import MinMaxScaler

sequence_length = 25  # You can adjust this based on your data

# Create sequences and corresponding target values
sequences = []
targets = []

# Initialize scalers for sequences and targets
seq_scaler = MinMaxScaler()
target_scaler = MinMaxScaler()

for i in range(len(data) - sequence_length):
    seq = data[['VDS_1_Flow (Veh/Hour)', 'VDS_1_Occupancy (%)']].iloc[i:i+sequence_length].values
    target = data['VDS_1_Speed (mph)'].iloc[i+sequence_length]  # Change 'Vehicles' to 'Speed'
    sequences.append(seq)
    targets.append(target)

# Convert sequences and targets to NumPy arrays
X = np.array(sequences)
y = np.array(targets)

# Fit and transform the data using the scalers
X = seq_scaler.fit_transform(X.reshape(-1, 2)).reshape(X.shape)
y = target_scaler.fit_transform(y.reshape(-1, 1)).reshape(y.shape)

data.info

"""# **Train-Test Split**"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""# **GRU Model**"""

from tensorflow import keras

model_gru = keras.Sequential([
    # First GRU layer with 256 units and 'relu' activation
    keras.layers.GRU(units=256, activation='relu', return_sequences=True, input_shape=(sequence_length, 2)),

    # Second GRU layer with 128 units and 'relu' activation
    keras.layers.GRU(units=128, activation='relu', return_sequences=True),

    # Additional GRU layers can be added as needed

    # Flatten layer to prepare for Dense layers
    keras.layers.Flatten(),

    # First Dense layer with 64 units and 'relu' activation
    keras.layers.Dense(units=64, activation='relu'),

    # Second Dense layer with 32 units and 'relu' activation
    keras.layers.Dense(units=32, activation='relu'),

    # Output Dense layer with 1 unit (for regression tasks)
    keras.layers.Dense(units=1)
])
# Print the model summary
model_gru.summary()


# Compile the model
model_gru.compile(optimizer='adam', loss='mean_squared_error')

import matplotlib.pyplot as plt

# Train the GRU model and store the history
history = model_gru.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1)

# Get the training and validation loss from the history
loss = history.history['loss']
val_loss = history.history['val_loss']

# Plot the training and validation loss
plt.figure(figsize=(10, 6))
plt.plot(loss, label='Training Loss', color='blue')
plt.plot(val_loss, label='Validation Loss', color='red')
plt.title('Training and Validation Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

"""# **RNN Model**"""

from tensorflow import keras

model_rnn = keras.Sequential([
    # First SimpleRNN layer with 256 units and 'relu' activation
    keras.layers.SimpleRNN(units=256, activation='relu', return_sequences=True, input_shape=(sequence_length, 2)),

    # Second SimpleRNN layer with 128 units and 'relu' activation
    keras.layers.SimpleRNN(units=128, activation='relu', return_sequences=True),

    # Additional SimpleRNN layers can be added as needed

    # Flatten layer to prepare for Dense layers
    keras.layers.Flatten(),

    # First Dense layer with 64 units and 'relu' activation
    keras.layers.Dense(units=64, activation='relu'),

    # Second Dense layer with 32 units and 'relu' activation
    keras.layers.Dense(units=32, activation='relu'),

    # Output Dense layer with 1 unit (for regression tasks)
    keras.layers.Dense(units=1)
])

model_rnn.summary()

# Compile the model
model_rnn.compile(optimizer='adam', loss='mean_squared_error')

# Print the model summary
model_rnn.summary()

# Compile the model
model_rnn.compile(optimizer='adam', loss='mean_squared_error')

# Train the RNN model
history=model_rnn.fit(X_train, y_train, epochs=100, batch_size=32)

# Get the training and validation loss from the history
loss = history.history['loss']


# Plot the training and validation loss
plt.figure(figsize=(10, 6))
plt.plot(loss, label='Training Loss', color='blue')

plt.title('Training Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

loss = history.history['loss']


# Plot the training and validation loss
plt.figure(figsize=(10, 6))
plt.plot(loss, label='Training Loss', color='blue')

plt.title('Training Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

"""# **LSTM Model**"""

from tensorflow import keras

model_lstm = keras.Sequential([
    # First LSTM layer with 256 units and 'relu' activation
    keras.layers.LSTM(units=256, activation='relu', return_sequences=True, input_shape=(sequence_length, 2)),

    # Second LSTM layer with 128 units and 'relu' activation
    keras.layers.LSTM(units=128, activation='relu', return_sequences=True),

    # Additional LSTM layers can be added as needed

    # Flatten layer to prepare for Dense layers
    keras.layers.Flatten(),

    # First Dense layer with 64 units and 'relu' activation
    keras.layers.Dense(units=64, activation='relu'),

    # Second Dense layer with 32 units and 'relu' activation
    keras.layers.Dense(units=32, activation='relu'),

    # Output Dense layer with 1 unit (for regression tasks)
    keras.layers.Dense(units=1)
])

# Compile the model
model_lstm.compile(optimizer='adam', loss='mean_squared_error')

# Print the model summary
model_lstm.summary()

# Train the LSTM model
history=model_lstm.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1)
# Get the training and validation loss from the history
loss = history.history['loss']
val_loss = history.history['val_loss']

# Plot the training and validation loss
plt.figure(figsize=(10, 6))
plt.plot(loss, label='Training Loss', color='blue')
plt.plot(val_loss, label='Validation Loss', color='red')
plt.title('Training and Validation Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

"""# **Mean Squared Error**

Mean Squared Error is most reliable metrics which can help us to determine if model is really ok or not
"""

# Make predictions using the test dataset
y_pred_rnn = model_rnn.predict(X_test)
y_pred_lstm = model_lstm.predict(X_test)
y_pred_gru = model_gru.predict(X_test)

# Evaluate the models (you can choose different metrics)
from sklearn.metrics import mean_squared_error
mse_rnn = mean_squared_error(y_test, y_pred_rnn)
mse_lstm = mean_squared_error(y_test, y_pred_lstm)
mse_gru = mean_squared_error(y_test, y_pred_gru)

print(f"Mean Squared Error (RNN): {mse_rnn}")
print(f"Mean Squared Error (LSTM): {mse_lstm}")
print(f"Mean Squared Error (GRU): {mse_gru}")

import matplotlib.pyplot as plt

# Mean Squared Error values
mse_values = {
    'RNN': 20.624012286560127,
    'LSTM': 41.51218544979721,
    'GRU': 15.755262226862396
}

# Extract the model names and MSE values
models = list(mse_values.keys())
mse = list(mse_values.values())

# Create a bar graph
plt.bar(models, mse, color='royalblue')
plt.xlabel('Model')
plt.ylabel('Mean Squared Error (MSE)')
plt.title('MSE Comparison')
plt.ylim(0, max(mse) + 100)  # Set the y-axis limit based on the maximum MSE value
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Display the values on top of each bar
for i, v in enumerate(mse):
    plt.text(i, v + 10, f'{v:.2f}', ha='center', va='bottom')

# Show the plot
plt.show()

"""# **R-Squared Score**

The R-squared metrics ranges between 0 to 1 , 1 means the model is best fit while 0 means worst, clearly seen below here also RNN performs best whith almost 77% accuracy indicating to be best fit, we must note that Time Series data also has a lot of noise suppose there is National Holiday or there is some festival certain days Traffic Flows may be unpredictable
"""

from sklearn.metrics import r2_score

# Calculate R-squared (coefficient of determination)
r2_rnn = r2_score(y_test, y_pred_rnn)
r2_lstm = r2_score(y_test, y_pred_lstm)
r2_gru = r2_score(y_test, y_pred_gru)

print(f"R-squared (RNN): {r2_rnn}")
print(f"R-squared (LSTM): {r2_lstm}")
print(f"R-squared (GRU): {r2_gru}")

import matplotlib.pyplot as plt

# R-squared values
r2_values = [r2_rnn, r2_lstm, r2_gru]
models = ['RNN', 'LSTM', 'GRU']

# Create a bar chart
plt.bar(models, r2_values, color='blue')
plt.xlabel('Models')
plt.ylabel('R-squared (R2) Value')
plt.title('R-squared Value for Different Models')
plt.ylim(0, 1)  # Set the y-axis limits from 0 to 1
plt.show()

"""# **Root Mean Squared Error**"""

import numpy as np

# Calculate the squared differences
squared_diff_rnn = (y_test - y_pred_rnn) ** 2
squared_diff_lstm = (y_test - y_pred_lstm) ** 2
squared_diff_gru = (y_test - y_pred_gru) ** 2

# Calculate the mean squared difference
mse_rnn = np.mean(squared_diff_rnn)
mse_lstm = np.mean(squared_diff_lstm)
mse_gru = np.mean(squared_diff_gru)

# Calculate RMSE
rmse_rnn = np.sqrt(mse_rnn)
rmse_lstm = np.sqrt(mse_lstm)
rmse_gru = np.sqrt(mse_gru)

print(f"Root Mean Squared Error (RNN): {rmse_rnn}")
print(f"Root Mean Squared Error (LSTM): {rmse_lstm}")
print(f"Root Mean Squared Error (GRU): {rmse_gru}")

import numpy as np
import matplotlib.pyplot as plt

# RMSE values for each model
rmse_values = [rmse_rnn, rmse_lstm, rmse_gru]
models = ['RNN', 'LSTM', 'GRU']

# Create a bar plot
plt.bar(models, rmse_values, color=['blue', 'green', 'red'])
plt.xlabel('Model')
plt.ylabel('RMSE')
plt.title('Root Mean Squared Error (RMSE) for Different Models')
plt.show()

"""# **Mean Absolute Error**"""

from sklearn.metrics import mean_absolute_error

# Calculate Mean Absolute Error (MAE)
mae_rnn = mean_absolute_error(y_test, y_pred_rnn)
mae_lstm = mean_absolute_error(y_test, y_pred_lstm)
mae_gru = mean_absolute_error(y_test, y_pred_gru)

print(f"Mean Absolute Error (RNN): {mae_rnn}")
print(f"Mean Absolute Error (LSTM): {mae_lstm}")
print(f"Mean Absolute Error (GRU): {mae_gru}")

import matplotlib.pyplot as plt

# MAE values
mae_values = [mae_rnn, mae_lstm, mae_gru]
models = ['RNN', 'LSTM', 'GRU']

# Create a bar chart
plt.bar(models, mae_values, color='green')
plt.xlabel('Models')
plt.ylabel('Mean Absolute Error (MAE)')
plt.title('Mean Absolute Error for Different Models')
plt.show()

"""# **Predicting the future values**"""

#what we observe is the timestamps are changing after an hour each so we will do the prediction accordingly
#the task is to predict the speed after 7 days that means we want 168 predictions
# Make predictions using the test dataset
y_pred_rnn = model_rnn.predict(X_test)
y_pred_lstm = model_lstm.predict(X_test)
y_pred_gru = model_gru.predict(X_test)

#7 days
print("Actual Speed : ", y_test[150])
print("Prediction by RNN : ",y_pred_rnn[150])
print("Prediction by LSTM : ",y_pred_lstm[150])
print("Prediction by GRU : ",y_pred_gru[150])

#3 days
print("Actual Speed : ", y_test[73])
print("Prediction by RNN : ",y_pred_rnn[73])
print("Prediction by LSTM : ",y_pred_lstm[73])
print("Prediction by GRU : ",y_pred_gru[73])

#5 days
print("Actual Speed : ", y_test[120])
print("Prediction by RNN : ",y_pred_rnn[120])
print("Prediction by LSTM : ",y_pred_lstm[120])
print("Prediction by GRU : ",y_pred_gru[120])

"""Hence we conclude that model gave satisfactory results of predicting next 150 hours that is one week time ,
Hence model is good

# **Conclusion and Findings**

>The data set collected was from PEMS CALTECH website which has collected data
using IOT sensors

>The dataset presented will be used to train a Deep Learning model which aims to estimate the speeds of the vehicles in future

>To estimate the future speeds we have done a train-test split in which train is the values till this date (assume to be in past) and test contains the future values

>The dataset being used for prediction the most appropriate error metrics will be Mean-Squared Error

>We have taken 25 as the sequence length beacaue after every 20 to 25 units of time the sequence of speed repeats itself

>It is interesting to note that the mean-squared error was observed to be minimum in the RNN model while the R-squared score was observed to be best in RNN model

>Overall the RNN model seems to be more suitable in all cases according to findings in this report
"""